{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgwlee96/Fall2024/blob/main/DATA_255_Fall24_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Deep Learning-Based Recommendation (10 Points)"
      ],
      "metadata": {
        "id": "sLYAlpcUqGHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the paper Wide and Deep Learning for Recommender Systems.\n",
        "Download the files anime-dataset-2023.csv, users-details-2023.csv, users-score- 2023.csv\n",
        "from the following link: https://www.kaggle.com/datasets/dbdmobile/myanimelist-dataset\n",
        "Based on the architecture described in the paper, build your own Wide and Deep\n",
        "Recommender system for the Anime Dataset. Your model should learn the features of each\n",
        "user and anime, not just the associated ID numbers. Utilize an 80/20 train-test split and record\n",
        "your model’s prediction accuracy."
      ],
      "metadata": {
        "id": "Z2a5_VhsqK0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yipOVD4Dm1ls",
        "outputId": "4e0c50cb-00f2-48ee-fef8-e7f8eb66fe33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "anime = None\n",
        "users = None\n",
        "scores = None\n",
        "\n",
        "parent_dir = '/content/drive/Shareddrives/DATA 255 Lab Group 22/dataset/part1/'\n",
        "\n",
        "if anime is None or users is None or scores is None:\n",
        "# Load datasets\n",
        "  anime = pd.read_csv(parent_dir + \"anime-dataset-2023.csv\")\n",
        "  users = pd.read_csv(parent_dir + \"users-details-2023.csv\")\n",
        "  scores = pd.read_csv(parent_dir + \"users-score-2023.csv\")\n",
        "else:\n",
        "  print(\"Datasets already loaded\")"
      ],
      "metadata": {
        "id": "09oNq4N2wBKE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# anime"
      ],
      "metadata": {
        "id": "Rd3W9W6UxpvV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# users"
      ],
      "metadata": {
        "id": "EZXDbA4WxqCs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scores"
      ],
      "metadata": {
        "id": "u6jg0TbQxquN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging datasets to get a full interaction table\n",
        "data = pd.merge(pd.merge(scores, users[['Username', 'Mean Score', 'Location']], on='Username'),\n",
        "                anime[['anime_id', 'Score', 'Genres', 'Type', 'Episodes']], on='anime_id')\n",
        "\n"
      ],
      "metadata": {
        "id": "lS9r9Io7-sSP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering - handling categorical features\n",
        "data['user_id'] = data['user_id'].astype('category').cat.codes\n",
        "data['anime_id'] = data['anime_id'].astype('category').cat.codes\n",
        "data['Genres'] = data['Genres'].astype('category').cat.codes\n",
        "data['Type'] = data['Type'].astype('category').cat.codes\n",
        "data['Location'] = data['Location'].astype('category').cat.codes\n",
        "\n",
        "# Handle missing values\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Train-test split\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "VgnXa_t9IDts"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that 'Score' column is numeric, and handle non-numeric values by filling with a default (e.g., 0.0)\n",
        "data['Score'] = pd.to_numeric(data['Score'], errors='coerce').fillna(0.0)\n",
        "\n",
        "# Handle any other columns that might have non-numeric values\n",
        "data['Mean Score'] = pd.to_numeric(data['Mean Score'], errors='coerce').fillna(0.0)\n",
        "\n",
        "# Split train and test\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset class\n",
        "class AnimeDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = torch.tensor(df['user_id'].values, dtype=torch.long)\n",
        "        self.anime = torch.tensor(df['anime_id'].values, dtype=torch.long)\n",
        "        self.scores = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
        "        self.mean_score = torch.tensor(df['Mean Score'].values, dtype=torch.float32)\n",
        "        self.anime_score = torch.tensor(df['Score'].values, dtype=torch.float32)\n",
        "        self.genres = torch.tensor(df['Genres'].values, dtype=torch.long)\n",
        "        self.type = torch.tensor(df['Type'].values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.scores)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.users[idx], self.anime[idx], self.scores[idx],\n",
        "                self.mean_score[idx], self.anime_score[idx], self.genres[idx], self.type[idx])\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = AnimeDataset(train_data)\n",
        "test_dataset = AnimeDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "YS-Op_4awhIl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class WideAndDeep(nn.Module):\n",
        "    def __init__(self, num_users, num_anime, num_genres, num_types, embedding_dim):\n",
        "        super(WideAndDeep, self).__init__()\n",
        "\n",
        "        # Wide component (linear)\n",
        "        # The input features include user_id, anime_id, genres, type, mean_score, anime_score\n",
        "        # So we need 6 input features in the wide component.\n",
        "        self.wide = nn.Linear(6, 1)\n",
        "\n",
        "        # Deep component (embeddings for categorical features)\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.anime_embedding = nn.Embedding(num_anime, embedding_dim)\n",
        "        self.genre_embedding = nn.Embedding(num_genres, embedding_dim)\n",
        "        self.type_embedding = nn.Embedding(num_types, embedding_dim)\n",
        "\n",
        "        # Deep neural network layers\n",
        "        self.deep = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 4 + 2, 128),  # 4 embeddings + 2 scalar features (mean_score, anime_score)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_id, anime_id, mean_score, anime_score, genres, type):\n",
        "        # Wide component\n",
        "        wide_input = torch.cat([\n",
        "            user_id.float().unsqueeze(1),  # Unsqueeze to make it 2D\n",
        "            anime_id.float().unsqueeze(1),  # Unsqueeze to make it 2D\n",
        "            mean_score.unsqueeze(1),  # Already 1D, needs unsqueeze for 2D\n",
        "            anime_score.unsqueeze(1),  # Already 1D, needs unsqueeze for 2D\n",
        "            genres.float().unsqueeze(1),  # Unsqueeze to make it 2D\n",
        "            type.float().unsqueeze(1)  # Unsqueeze to make it 2D\n",
        "        ], dim=1)\n",
        "\n",
        "        # Ensure that the input dimension is correct for the wide layer\n",
        "        wide_output = self.wide(wide_input)\n",
        "\n",
        "        # Deep component\n",
        "        user_embed = self.user_embedding(user_id)\n",
        "        anime_embed = self.anime_embedding(anime_id)\n",
        "        genre_embed = self.genre_embedding(genres)\n",
        "        type_embed = self.type_embedding(type)\n",
        "\n",
        "        # Concatenate embeddings with continuous features\n",
        "        deep_input = torch.cat([user_embed, anime_embed, genre_embed, type_embed,\n",
        "                                mean_score.unsqueeze(1), anime_score.unsqueeze(1)], dim=1)\n",
        "        deep_output = self.deep(deep_input)\n",
        "\n",
        "        # Combine wide and deep outputs\n",
        "        out = wide_output + deep_output\n",
        "        return out.squeeze()\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "model = WideAndDeep(num_users=len(data['user_id'].unique()),\n",
        "                    num_anime=len(data['anime_id'].unique()),\n",
        "                    num_genres=len(data['Genres'].unique()),\n",
        "                    num_types=len(data['Type'].unique()),\n",
        "                    embedding_dim=10)\n",
        "\n",
        "# Device configuration (CPU or GPU)\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for user_id, anime_id, score, mean_score, anime_score, genres, type in train_loader:\n",
        "            user_id, anime_id, score, mean_score, anime_score, genres, type = \\\n",
        "                user_id.to(device), anime_id.to(device), score.to(device), \\\n",
        "                mean_score.to(device), anime_score.to(device), genres.to(device), type.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(user_id, anime_id, mean_score, anime_score, genres, type)\n",
        "            loss = criterion(outputs, score)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for user_id, anime_id, score, mean_score, anime_score, genres, type in test_loader:\n",
        "            user_id, anime_id, score, mean_score, anime_score, genres, type = \\\n",
        "                user_id.to(device), anime_id.to(device), score.to(device), \\\n",
        "                mean_score.to(device), anime_score.to(device), genres.to(device), type.to(device)\n",
        "            outputs = model(user_id, anime_id, mean_score, anime_score, genres, type)\n",
        "            loss = criterion(outputs, score)\n",
        "            total_loss += loss.item()\n",
        "    print(f\"Test Loss: {total_loss/len(test_loader)}\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "jbQV-LQQwrsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1dc0a8-3686-4e8a-bed5-3741c2e197a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1167870.9916431513\n",
            "Epoch 2, Loss: 29.330061409576796\n",
            "Epoch 3, Loss: 29.32375280246963\n",
            "Epoch 4, Loss: 29.000527335439802\n",
            "Epoch 5, Loss: 28.79231848312474\n",
            "Test Loss: 4.3003381208942315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: Image Classification with Deep Learning (40 Points)"
      ],
      "metadata": {
        "id": "pQTdxg6DqPVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Download the Sports Image Dataset from the given link:\n",
        "https://www.kaggle.com/datasets/sidharkal/sports-image-classification/data\n",
        "This dataset consists of labeled images belonging to the following sports classes:\n",
        "cricket, wrestling, tennis, badminton, soccer, swimming, and karate.\n"
      ],
      "metadata": {
        "id": "E2cMXqfRqQT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transforms for data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root='sports-image/train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='sports-image/test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "A7Og_979qPUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##2. Explain in your own words: (7 points)"
      ],
      "metadata": {
        "id": "Mod68P5VqPuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "a. Gradient Descent (1 point)\n"
      ],
      "metadata": {
        "id": "lDbbW_A4qX5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent is an optimization algorithm used to minimize a loss function by iteratively moving in the direction of the steepest descent (i.e., the negative gradient of the loss function). The basic idea is to adjust the model's parameters (e.g., weights and biases) in small steps, based on the gradient of the loss function with respect to these parameters. The process continues until convergence, ideally reaching a point where the loss function is minimized.\n",
        "\n"
      ],
      "metadata": {
        "id": "lmynw6kAFfwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. List 3 regularization techniques and explain (2 points)\n"
      ],
      "metadata": {
        "id": "s0goXnjvqY-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2 Regularization (Ridge): This technique adds the squared sum of all model parameters (weights) to the loss function. The model is penalized for having large weights, which helps prevent overfitting by encouraging smaller weights. The regularization term is scaled by a hyperparameter\n",
        "𝜆\n",
        "λ.\n",
        "\n",
        "Equation:\n",
        "\n",
        "Loss=Original Loss+λ∑w\n",
        "2\n",
        "\n",
        "\n",
        "L1 Regularization (Lasso): In L1 regularization, the absolute value of the weights is added to the loss function. This can lead to sparse solutions where some weights are reduced to exactly zero, which can be helpful for feature selection.\n",
        "\n",
        "Equation:\n",
        "\n",
        "Loss=Original Loss+λ∑∣w∣\n",
        "\n",
        "Dropout: Dropout is a technique where a random subset of neurons is \"dropped out\" during each forward pass during training. This forces the network to not rely too heavily on any individual neuron, improving generalization and reducing overfitting. Dropout is typically used only during training, and the full network is used during testing."
      ],
      "metadata": {
        "id": "9T6JmkbsFkVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Activation functions (1 point)\n"
      ],
      "metadata": {
        "id": "4RyHbvWjqZur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions introduce non-linearity into neural networks, allowing them to model complex data patterns. Common activation functions include:\n",
        "\n",
        "ReLU (Rectified Linear Unit):\n",
        "f(x)=max(0,x), where all negative values are set to zero.  \n",
        "Sigmoid:\n",
        "f(x)=1/(1+e^(−x))\n",
        " , which outputs values between 0 and 1.  \n",
        "Tanh:\n",
        "f(x)=tanh(x), which outputs values between -1 and 1.  \n",
        "Activation functions help neurons \"fire\" based on their input, enabling deep networks to learn complex patterns."
      ],
      "metadata": {
        "id": "-h_ICl4WF0Bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Loss function and Back Propagation (2 points)\n"
      ],
      "metadata": {
        "id": "nLZzQgHEqa2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function: A loss function measures how well the model's predictions match the actual target values. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification. The goal is to minimize the loss function during training.\n",
        "\n",
        "Back Propagation: Back propagation is the process of calculating the gradient of the loss function with respect to each weight in the neural network, using the chain rule. It starts from the output layer and works its way backward through the network, updating the weights to reduce the loss by applying gradient descent. This process is repeated iteratively during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "P4xjtdvvGicQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Epochs, Iterations, and Batch size (2 points)\n",
        "\n"
      ],
      "metadata": {
        "id": "JEhu6nuPqcE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch: One epoch is a complete pass of the entire training dataset through the model. It indicates how many times the model has seen the full dataset during training.\n",
        "\n",
        "Iteration: An iteration is a single update of the model’s parameters, usually based on a single batch of data. The number of iterations in one epoch equals the total number of training examples divided by the batch size.\n",
        "\n",
        "Batch Size: Batch size is the number of training examples processed before updating the model’s parameters. A smaller batch size results in more frequent updates but increases variance in the updates, while a larger batch size reduces variance but can slow down the learning process."
      ],
      "metadata": {
        "id": "y0xu_392GlYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Visualize/summarize the data (12 points)\n"
      ],
      "metadata": {
        "id": "ejYSr1wzqUT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.Number of images in the training and testing set and number of classes in the\n",
        "target variable (1 point)\n"
      ],
      "metadata": {
        "id": "mJ15g6XXqfTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.Number of images per class (1 point)\n"
      ],
      "metadata": {
        "id": "dQHUgmUnqiDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count images per class\n",
        "class_counts = {class_name: len(os.listdir(os.path.join(train_dir, class_name))) for class_name in os.listdir(train_dir)}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(class_counts.keys(), class_counts.values())\n",
        "plt.title('Number of Images per Class')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F-ZAbbUCrnLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.Number of pixels in the first 2 images of each class (Height and width individually)\n",
        "(2 points)\n"
      ],
      "metadata": {
        "id": "YPs038Ghqj67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_sizes = {}\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    images = os.listdir(class_path)[:2]  # Get the first two images\n",
        "    image_sizes[class_name] = []\n",
        "\n",
        "    for image_file in images:\n",
        "        with Image.open(os.path.join(class_path, image_file)) as img:\n",
        "            width, height = img.size\n",
        "            image_sizes[class_name].append((width, height))\n",
        "\n",
        "print(\"Image Sizes (Width, Height) for the First 2 Images of Each Class:\")\n",
        "print(image_sizes)"
      ],
      "metadata": {
        "id": "MgBWWRIVrnj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Display at least 3 images of each class (3 points)\n"
      ],
      "metadata": {
        "id": "fFhAVS2-qmcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(class_name, num_images=3):\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    images = os.listdir(class_path)[:num_images]\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, image_file in enumerate(images):\n",
        "        img = Image.open(os.path.join(class_path, image_file))\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(class_name)\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for class_name in os.listdir(train_dir):\n",
        "    display_images(class_name)"
      ],
      "metadata": {
        "id": "9I87gYpqrn1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Apply data augmentation and other image preprocessing and plot the sample of\n",
        "processed images. (3 points)\n"
      ],
      "metadata": {
        "id": "LHXzghrOqoVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define data augmentation and preprocessing\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Visualize augmented images\n",
        "def visualize_augmentation(class_name):\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    images = os.listdir(class_path)[:3]  # Get first 3 images\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, image_file in enumerate(images):\n",
        "        img = Image.open(os.path.join(class_path, image_file))\n",
        "        img_transformed = data_transforms(img)\n",
        "\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.imshow(img_transformed.permute(1, 2, 0))  # Permute for correct color channels\n",
        "        plt.title(f'Augmented {class_name}')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for class_name in os.listdir(train_dir):\n",
        "    visualize_augmentation(class_name)"
      ],
      "metadata": {
        "id": "ApX18YgfroUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Train a neural network (21 points)\n"
      ],
      "metadata": {
        "id": "nn6fppatqqrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.Decide the number of layers and neurons in each layer (2 points)\n"
      ],
      "metadata": {
        "id": "XCBxxH5Vqsh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=128 * 128 * 3, out_features=512)  # Flattened input size\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
        "        self.fc3 = nn.Linear(in_features=256, out_features=num_classes)  # Output layer for classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "VQrftX0Xrozi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.Try different number of epochs and batch sizes (2 points)\n"
      ],
      "metadata": {
        "id": "3lhLkAQKqt36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_epochs_list = [10, 20, 30]  # Different epochs to try\n",
        "batch_sizes = [16, 32, 64]  # Different batch sizes to try"
      ],
      "metadata": {
        "id": "qiCJ4bL3rpLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Try out different activation functions (explain each one you used) (3 points)\n"
      ],
      "metadata": {
        "id": "je9SYnxXqvSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using ReLU, Sigmoid, and Tanh\n",
        "class CustomNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(128 * 128 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))  # ReLU activation\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "eThan3CPrpfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU (Rectified Linear Unit): Outputs the input directly if positive; otherwise, it outputs zero. This allows the model to learn non-linear patterns and avoids the vanishing gradient problem for positive inputs.\n",
        "Sigmoid: Outputs values between 0 and 1, making it useful for binary classification. However, it can suffer from the vanishing gradient problem for extreme values."
      ],
      "metadata": {
        "id": "h29qdhH9nlIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d.Try at least three different regularizations (3 points)\n"
      ],
      "metadata": {
        "id": "7bmt7EoRqwvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 Regularization\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=0.01)  # L2 regularization\n",
        "\n",
        "# L1 Regularization (manual implementation)\n",
        "def l1_loss(model, lambda_l1=0.01):\n",
        "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "    return lambda_l1 * l1_norm\n",
        "\n",
        "# Dropout (included in the model definition)\n",
        "self.dropout = nn.Dropout(p=0.5)"
      ],
      "metadata": {
        "id": "RhRdn07crpyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e.Try different loss functions (3 points)\n"
      ],
      "metadata": {
        "id": "0L1xV2KBqyOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions to try\n",
        "loss_fn1 = nn.CrossEntropyLoss()  # Common for multi-class classification\n",
        "loss_fn2 = nn.BCEWithLogitsLoss()  # For binary classification tasks\n",
        "loss_fn3 = nn.MSELoss()  # Used for regression tasks"
      ],
      "metadata": {
        "id": "y0DLFaOfrqCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f. Try different optimization algorithms (such as Gradient Descent, Adam, etc.) (4 points)\n"
      ],
      "metadata": {
        "id": "4g16tAlcqzWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Different optimizers\n",
        "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "SNhgrlyvrqTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "g. Decide your best-performing model based on both time and accuracy. (1\n",
        "point)\n"
      ],
      "metadata": {
        "id": "smmI0v6bq0m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After training different models and comparing their performance\n",
        "best_model = 'Adam with L2 Regularization'  # Example\n",
        "print(f'Best Performing Model: {best_model}')"
      ],
      "metadata": {
        "id": "tU7Ah-jGrqsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "h. Create a graph of loss vs epochs for training and testing set. (1 point)\n"
      ],
      "metadata": {
        "id": "dEbJ4SG-q1vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss tracking\n",
        "train_losses = []  # Store training losses\n",
        "test_losses = []  # Store testing losses\n",
        "\n",
        "# Training loop (pseudocode)\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model and calculate loss\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
        "plt.plot(range(num_epochs), test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs. Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aV3Hc3QIrrB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i.Create a graph of f1 score vs epochs for training and testing set. (1 point)\n"
      ],
      "metadata": {
        "id": "NPBZOT5hq3Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# F1 score tracking\n",
        "train_f1_scores = []  # Store F1 scores for training set\n",
        "test_f1_scores = []  # Store F1 scores for testing set\n",
        "\n",
        "# Calculate F1 scores in the training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Calculate F1 scores\n",
        "    train_f1_scores.append(train_f1)\n",
        "    test_f1_scores.append(test_f1)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(range(num_epochs), train_f1_scores, label='Train F1 Score')\n",
        "plt.plot(range(num_epochs), test_f1_scores, label='Test F1 Score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score vs. Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aj208eVbrrdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "j.Calculate the number of trainable parameters in your final model. (1 point)"
      ],
      "metadata": {
        "id": "esq9TV6Gq4N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Calculate and print the number of trainable parameters\n",
        "num_params = count_parameters(best_model)\n",
        "print(f'Number of Trainable Parameters in Final Model: {num_params}')"
      ],
      "metadata": {
        "id": "l5hh22DKrr0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 3: Object Detection (50 Points)"
      ],
      "metadata": {
        "id": "jXFexy0CqZhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the differences between object detection, image classification, and image\n",
        "segmentation. (3 points)\n"
      ],
      "metadata": {
        "id": "TMeuwkdYqf0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Classification: This task involves categorizing an entire image into a single label or class. For example, identifying an image as \"dog\" or \"cat\" without any information about the position of the object in the image.\n",
        "\n",
        "Object Detection: Object detection extends image classification by not only identifying objects within an image but also localizing them with bounding boxes. For instance, in an image with multiple objects, the model will output both the classes and their respective positions.\n",
        "\n",
        "Image Segmentation: This technique goes a step further by classifying each pixel in the image. In semantic segmentation, each pixel is assigned a class label, while in instance segmentation, each object instance is differentiated. For example, in an image with dogs and cats, each pixel belonging to a dog would be labeled as \"dog\" and each pixel belonging to a cat would be labeled as \"cat.\""
      ],
      "metadata": {
        "id": "rb62D8VxoFip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the architectures and differences between R-CNN, Fast R-CNN, and Faster\n",
        "R-CNN. (4 points)\n"
      ],
      "metadata": {
        "id": "iUmJUfUyrH0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-CNN (Regions with CNN features): R-CNN first generates region proposals using a selective search and then uses a CNN to extract features from each region. Each region is classified using a Support Vector Machine (SVM). The main drawback is the slow inference speed due to the need to run the CNN for each region proposal separately.\n",
        "\n",
        "Fast R-CNN: This improvement allows the CNN to process the entire image to extract features, creating a feature map. Region proposals are then applied to this feature map, significantly reducing the number of CNN evaluations needed. It uses a softmax layer for classification and a bounding box regression layer for refining the region proposals. This makes it faster than R-CNN.\n",
        "\n",
        "Faster R-CNN: This model introduces a Region Proposal Network (RPN) that shares convolutional features with the detection network, allowing the model to propose regions of interest directly. This integration improves the speed and efficiency of the object detection process compared to both R-CNN and Fast R-CNN."
      ],
      "metadata": {
        "id": "-73hPFo4oH_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain what’s U-net. (5 points)\n"
      ],
      "metadata": {
        "id": "dlJFB-33rJ0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "U-Net is a convolutional neural network architecture primarily used for image segmentation tasks, particularly in biomedical image segmentation. The architecture consists of a contracting path (encoder) and an expanding path (decoder):\n",
        "\n",
        "Encoder: The encoder captures context through successive convolutional and pooling layers, downsampling the image to learn rich feature representations.\n",
        "\n",
        "Decoder: The decoder upsamples the feature maps and combines them with corresponding feature maps from the encoder through skip connections. This helps preserve spatial information that might be lost during downsampling.\n",
        "\n",
        "The U-Net architecture is notable for its symmetrical design and effective handling of small datasets, making it a popular choice for segmentation tasks."
      ],
      "metadata": {
        "id": "-ClP9O4qoKpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. List at least 3 widely used metrics in the object detection industry and explain them in\n",
        "detail. (3 points)\n"
      ],
      "metadata": {
        "id": "17WxQf_6rLjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Average Precision (mAP): This metric evaluates the precision and recall of the detection model across multiple Intersection over Union (IoU) thresholds. It calculates the average precision for each class, then takes the mean across all classes. mAP is crucial for assessing how well a model detects objects across different classes.\n",
        "\n",
        "Intersection over Union (IoU): IoU measures the overlap between the predicted bounding box and the ground truth bounding box. It is defined as the area of intersection divided by the area of union. A higher IoU indicates better localization. It is often used as a threshold to determine if a detection is considered correct.\n",
        "\n",
        "Precision and Recall:\n",
        "\n",
        "Precision indicates the accuracy of the positive predictions made by the model, calculated as the ratio of true positive detections to the total positive detections (true positives + false positives).  \n",
        "Recall measures the model's ability to identify all relevant instances, calculated as the ratio of true positives to the total actual positive instances (true positives + false negatives)."
      ],
      "metadata": {
        "id": "0pok7-SPoVeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain what’s Non-Maximum Suppression and how it works. (2 points)\n"
      ],
      "metadata": {
        "id": "XAUj6d3BrNlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Maximum Suppression (NMS) is a technique used in object detection to eliminate redundant overlapping bounding boxes. After an object detection model generates multiple bounding boxes for a detected object, NMS helps retain only the most confident box.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "1. For each detected object, sort the bounding boxes based on their confidence scores.  \n",
        "2. Select the box with the highest score and eliminate all other boxes that have an IoU above a predefined threshold with the selected box.\n",
        "3. Repeat the process with the next highest scoring box until all boxes are processed."
      ],
      "metadata": {
        "id": "wjtxz5j5oZKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Download Road Sign Dataset from the following link:\n",
        "https://drive.google.com/drive/folders/1yvzEtFDqodCUIssXIKMrQ_YtL_0gNHyA\n",
        "\n"
      ],
      "metadata": {
        "id": "K0Vc838FrPDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Perform necessary data transformation and augmentation steps (1 point)\n"
      ],
      "metadata": {
        "id": "PCS4CIk8rQXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define data transformations and augmentations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),   # Resize images to a standard size\n",
        "    transforms.RandomHorizontalFlip(), # Random horizontal flip\n",
        "    transforms.RandomRotation(10),     # Random rotation\n",
        "    transforms.ToTensor(),             # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalize\n",
        "])"
      ],
      "metadata": {
        "id": "xCnKOy11rRGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Plot some random images from the train, test, and validation set. (2 points)\n"
      ],
      "metadata": {
        "id": "PXfkPerMrRaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Function to display random images from the dataset\n",
        "def show_random_images(dataset, num_images=3):\n",
        "    fig, axs = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "    for ax in axs:\n",
        "        idx = random.randint(0, len(dataset) - 1)\n",
        "        img, label = dataset[idx]\n",
        "        ax.imshow(img.permute(1, 2, 0).numpy())  # Convert from CxHxW to HxWxC\n",
        "        ax.set_title(f'Label: {label}')\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display random images from the training set\n",
        "show_random_images(train_dataset)"
      ],
      "metadata": {
        "id": "uSCw5eE5rSRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Pick one model of your choice and implement it from scratch to perform object\n",
        "detection.\n"
      ],
      "metadata": {
        "id": "KkyiVaejrSpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO, SSD, or Faster R-CNN"
      ],
      "metadata": {
        "id": "O8_rx3Xxo3H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faster R-CNN\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "# Load pre-trained model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Load and preprocess image\n",
        "image = F.to_tensor(image).unsqueeze(0)\n",
        "model.eval()\n",
        "predictions = model(image)"
      ],
      "metadata": {
        "id": "blIhIPAOrToy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Compute the IOU of your results with the test set and print a few predicted images.\n"
      ],
      "metadata": {
        "id": "dIB8_AtDrT9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_iou(boxA, boxB):\n",
        "#     # Coordinates of intersection rectangle\n",
        "#     xA = max(boxA[0], boxB[0])\n",
        "#     yA = max(boxA[1], boxB[1])\n",
        "#     xB = min(boxA[2], boxB[2])\n",
        "#     yB = min(boxA[3], boxB[3])\n",
        "\n",
        "#     # Intersection area\n",
        "#     interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "\n",
        "#     # Union area\n",
        "#     boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "#     boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "#     unionArea = boxAArea + boxBArea - interArea\n",
        "\n",
        "#     # IoU calculation\n",
        "#     iou = interArea / unionArea\n",
        "#     return iou"
      ],
      "metadata": {
        "id": "K8IOUbOKrUwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute IoU\n",
        "def compute_iou(box1, box2):\n",
        "    x1_inter = max(box1[0], box2[0])\n",
        "    y1_inter = max(box1[1], box2[1])\n",
        "    x2_inter = min(box1[2], box2[2])\n",
        "    y2_inter = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
        "    return iou\n",
        "\n",
        "# Example usage\n",
        "# box1 = [x1, y1, x2, y2] format for predicted and ground truth boxes\n",
        "iou_score = compute_iou(box1, box2)\n",
        "print(f'IoU: {iou_score}')\n"
      ],
      "metadata": {
        "id": "g6DXbqyipEOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Use pre-trained models such as YOLOv8 for object detection and print the IOU.\n"
      ],
      "metadata": {
        "id": "rFQHXtCwrVJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute IoU\n",
        "def compute_iou(box1, box2):\n",
        "    x1_inter = max(box1[0], box2[0])\n",
        "    y1_inter = max(box1[1], box2[1])\n",
        "    x2_inter = min(box1[2], box2[2])\n",
        "    y2_inter = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
        "    return iou\n",
        "\n",
        "# Example usage\n",
        "# box1 = [x1, y1, x2, y2] format for predicted and ground truth boxes\n",
        "iou_score = compute_iou(box1, box2)\n",
        "print(f'IoU: {iou_score}')"
      ],
      "metadata": {
        "id": "2kFUL9_-rV7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Experiment with pre-trained models and show the IOU of the test data set. Show\n",
        "tables and graphs of how the results change.\n"
      ],
      "metadata": {
        "id": "Ynl5fPQOrWSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example data\n",
        "results_data = {\n",
        "    'Model': ['YOLOv3', 'Faster R-CNN', 'YOLOv8'],\n",
        "    'IoU Score': [0.65, 0.70, 0.75]\n",
        "}\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Plotting results\n",
        "plt.bar(results_df['Model'], results_df['IoU Score'])\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('IoU Score')\n",
        "plt.title('IoU Scores of Different Object Detection Models')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yx0_qs_IrXB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Submit a short report on the model you chose for Part 1 and why. Include the IOU results\n",
        "in the report. Discuss your observations and the hyperparameters you used for the\n",
        "model.\n",
        "(Step 9-13 30 points)"
      ],
      "metadata": {
        "id": "yJoJjkoWrXbC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "33ekHdDYrX0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}